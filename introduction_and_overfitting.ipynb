{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Administrivia\n",
    "\n",
    "# 1. Definitions\n",
    "To start thinking concretely about **machine learning**, we need to have a definition for it. \n",
    "\n",
    "## 1A. Machine\n",
    "What is the definition of the word **machine** in our context?  There are a few possibilties: Your Grandma’s sewing machine? \n",
    "<img src=\"images/sewing_machine.jpg\" style=\"width: 500px;\">\n",
    "A Pentium 2? A 3 to 1 pulley system? \n",
    "<img src=\"images/z_drag.jpg\" style=\"width: 500px;\">\n",
    "These are all examples of machines (as understood in common English).  However, when you hear the phrase Machine Learning in the news, they’re talking about something different. Indeed, the phrase **machine** is a little bit misleading. For everything we do, it would (in principle) be possible to do the same calculations on pen and paper. Instead we’re looking for sets of instructions that ingest some sort of input and produce some sort of outputs, in such a way that once we’ve settled on those instructions, we don’t need to continue to be involved. This has a name that is frequently used in Computer Science: an algorithm. In fact, a better name for this course might actually be algorithmic learning.\n",
    "\n",
    "## 1B. Learning\n",
    "So this is a course that deals in algorithms, and the mathematics that underlie those algorithms. But perhaps a more vacuous concept comes from the second word: learning. What does it mean to learn? Wikipedia defines it thusly: *Learning is the process of acquiring new or modifying existing knowledge, behaviors, skills, values, or preferences.* Do you like that definition? I think it’s stupid: modifying your behavior or skill isn’t learning, because you might be going backwards. For example, this notebook might be making you even more confused than you were to begin with, and you might be thinking about modifying your behavior in the direction of taking Liberal Studies 147: Underwater Basket Weaving instead (not to demean anyone, I did part of my undergrad at the Evergreen State College). In this case, you wouldn’t be getting better at the task you set out to do. And so there I’ve implicitly added a certain value judgement to this definition: learning needs to involve improvement and it needs to involve a fairly well defined subject over which you would like to learn. Improvement at what? Let’s consider that question with a little example.\n",
    "\n",
    "## 2. What to do with data?\n",
    "Imagine that you work for UNICEF or some other multinational non-profit human rights organization, and you're trying to decide where to allocate funding.  The word has come in that the European Union has provided a sizeable grant for humanitarian aid to the nation of Venezuela.  In particular, this funding has been earmarked for projects related to reducing the infant mortality rate.  However, the problem is that no one even knows what the infant mortality rate for Venezuela is to begin with!  Of course, this isn't true, but for the sake of argument, let's imagine that this information has never been collected.  We need to make a prediction about this number; **How shall we proceed?  What is your strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A.  The world is not random.\n",
    "Perhaps we can identify some proxy that might be related.  A reasonable *hypothesis* might be that per capita income is related to infant mortality rate.  Let's imagine that we have this information for Venezuela.  **Can we use that number to make a prediction?  What other information do we need in order to use it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can utilize our hypothesized relationship between PCI and IMR, we'll need to do some machine learning in order to determine the nature of that relationship.  How might we best quantify said relationship?  By looking at the relationship between PCI and IMR elsewhere.  Fortunately, a useful dataset for this purpose does exist, and I downloaded it from http://www.stat.ufl.edu/~winner/datasets.html, which has a ton of great datasets.  \n",
    "\n",
    "We can load these data using [pandas](https://pandas.pydata.org/), which we import here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in a text format with spaces as delimiters.  Pandas has a nice parsing utility that we can use to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/birthrate.dat',header=0,sep=r\"\\s{2,}\",engine='python',index_col=0)\n",
    "ven_data = data.loc['Venezuela']\n",
    "data = data.drop('Venezuela')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this dataset does actually include Venezuela, but I've split it from the training set for instructional purposes.    \n",
    "\n",
    "Let's have a look at what we just imported.  The Jupyter notebook has support for pandas data objects, which means they'll print nicely if we just say..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCI is in the second data column, while IMR is in the fourth.  However, tabular data is hard to interpret.  Let's plot instead, using the python package [matplotlib](https://matplotlib.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.figsize'] = [12,8]\n",
    "\n",
    "plt.plot(data['PCI'],data['Infant Mortality'],'ro')\n",
    "plt.xlabel('Per Capita Income')\n",
    "plt.ylabel('Infant Mortality Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That weird point out to the right looks kind of like an outlier, so let's drop it manually (American exceptionalism at work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('United States')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, from a computer's perspective, it's better to deal with small numbers rather than large ones (we'll see why later).  As such, we'd like to *normalize* these data so that both PCI and IMR scale from 0 to 1.  We'll save the amount that we scaled by, so we can always convert back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pci_min = data['PCI'].min()\n",
    "pci_range = data['PCI'].max() - pci_min\n",
    "x = (data['PCI'] - pci_min)/pci_range\n",
    "\n",
    "imr_min = data['Infant Mortality'].min()\n",
    "imr_range = data['Infant Mortality'].max() - imr_min\n",
    "y = (data['Infant Mortality'] - imr_min)/imr_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these data again, but rescaled and without the outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'ro')\n",
    "plt.xlabel('Relative Per Capita Income')\n",
    "plt.ylabel('Relative Infant Mortality Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B. Hypothesizing a model\n",
    "\n",
    "Clearly, there's some sort of relationship here that might be useful to us, for our stated goal, which is to make a prediction about the IMR in Venezuela.  Recall that we don't know that number, but *we do know its PCI*.  Let's plot that as a vertical line.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'ro')\n",
    "plt.axvline((ven_data['PCI'] - pci_min)/pci_range)\n",
    "plt.xlabel('Relative Per Capita Income')\n",
    "plt.ylabel('Relative Infant Mortality Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a moment and determine your strategy.  How will you use this information?**\n",
    "\n",
    "Make a list of potential options, but there's a common theme between all of these elements: we're going to construct a function of $x$, that yields a prediction about $y$.\n",
    "\n",
    "$$\n",
    "\\underbrace{y}_{\\text{prediction}} = \\underbrace{F}_{\\text{model}}(\\underbrace{x}_{\\text{feature}},\\ldots)\n",
    "$$\n",
    "These things have important names, and we're going to be revisiting them time after time for the remainder of the semester.  $y$ is going to be called the *prediction*.  What is $y$ for this problem?  $x$ in the Machine Learning literature is typically called a *feature*.  In this context, there's only one, but in the future there will be many, sometimes millions (this is where the term big data comes from).  \n",
    "\n",
    "## 2C. Incorporating Observations\n",
    "The above definition is actually incomplete.  There's an additional element to every model that we also need to write:\n",
    "$$\n",
    "\\underbrace{y}_{\\text{prediction}} = \\underbrace{F}_{\\text{model}}(\\underbrace{x}_{\\text{feature}},\\underbrace{w}_{\\text{parameter}})\n",
    "$$\n",
    "What is a parameter?  Let's look at a very specific model:\n",
    "$$\n",
    "y = w_0 + w_1 x\n",
    "$$\n",
    "What do these $w$ values do?  They change the behavior of the model.  And **why would we want to change the behavior of the model?  What happens if we select some arbitrary values for $w_0$ and $w_1$ and use the resulting model to make a prediction?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.plot(x,y,'ro')\n",
    "x_ven = (ven_data['PCI'] - pci_min)/pci_range\n",
    "\n",
    "w_0 = np.random.rand()*2 - 1\n",
    "w_1 = np.random.rand()*2 - 1\n",
    "\n",
    "plt.plot(x_ven,w_0 + w_1*x_ven,'bo')\n",
    "plt.plot(x,w_0 + w_1*x,'b-')\n",
    "\n",
    "plt.xlabel('Relative Per Capita Income')\n",
    "plt.ylabel('Relative Infant Mortality Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good prediction?\n",
    "\n",
    "**Prediction is not learning**.  We need to adjust our model in order to make better predictions, which *is* learning.  The parameters $w$ are knobs that we get to turn in order to make that adjustment, with the data that we have (in this case matched pairs of PCI and IMR).  **But how do we define good versus bad?  Get together with your neighbors and try to decide on some way to measure the model's predictive accuracy for the observations that exist**.  \n",
    "\n",
    "## 2D. The classic example\n",
    "What did you decide on?  The most common thing, and something you've probably all seen before is this:\n",
    "$$\n",
    "I(\\mathbf{x},\\underbrace{\\hat{\\mathbf{y}}}_{\\text{Observation}},\\mathbf{w}) = \\sum_{i=1}^m (F(x_i,\\mathbf{w}) - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "So now we come to *the* fundamental operation in machine learning.  Stated concretely, for this problem it might be the following:  Find $w_0,w_1$ such that \n",
    "$$\n",
    "\\sum_{i=1}^m (w_0 + w_1 x_i - \\hat{y}_i)^2\n",
    "$$\n",
    "is as small as possible.\n",
    "\n",
    "# 3. The general problem\n",
    "We can also state this more generally:  For a model \n",
    "$$\\underbrace{\\mathbf{y}}_{\\text{prediction}} = \\underbrace{F}_{\\text{model}}(\\underbrace{\\mathbf{x}}_{\\text{features}},\\underbrace{\\mathbf{w}}_{\\text{parameters}}),$$\n",
    "Minimize \n",
    "$$\\underbrace{I}_{\\text{Cost}}(\\mathbf{y},\\underbrace{\\hat{\\mathbf{y}}}_{\\text{data}},\\mathbf{w})$$\n",
    "with resepct to $\\mathbf{w}$.\n",
    "\n",
    "## 3A. The complexity of ML\n",
    "This is a very simple problem statement, but ML is a huge field growing daily with enormous workforce demand.  Where does the challenge and complexity come from.  Essentially, the richness of machine learning comes from trying to answer three fundamental questions:\n",
    "- **Model Selection**: What is $F(\\mathbf{x},\\mathbf{w})$ and what is $I(\\mathbf{y},\\hat{\\mathbf{y}},\\mathbf{w})$.\n",
    "\n",
    "- **Model Optimization**: How do we go about adjusting $\\mathbf{w}$ in order to minimize $I(\\mathbf{y},\\hat{\\mathbf{y}},\\mathbf{w})$?\n",
    "\n",
    "- **Model Validation**: How do know (and ensure) that our predictions are good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In class assignment 1: The machine learning recipe\n",
    "Now that we know what our objective is, let's try and solve the least squares problem (with a mind to the three questions posed above).  While we will return to the first question in a moment, let's assume that we've gone with the modelling strategy that we've determined above: that we will model the data as a straight line\n",
    "$$\\mathbf{y} = w_0 + w_1 \\mathbf{x},$$\n",
    "and that we will measure error using least-squares\n",
    "$$I(\\mathbf{y},\\hat{\\mathbf{y}},\\mathbf{w}) = \\sum_{i=1}^m (y_i - \\hat{y}_i)^2.$$\n",
    "\n",
    "## IC1A. The brute force method\n",
    "Now, let's try to answer the second question.  How do we go about adjusting $w_0$ and $w_1$ so that $I$ is as small as it can be?  One very straightforward way to do this is by direct search AKA brute force: test all possible combinations of parameters, and determine which one leads to a minimal $I$!  Your first in-class task is to do this.  \n",
    "- **Generate a large number of candidate values for $(w_0,w_1)$ (you could do this by defining a grid of values, or generating random samples, or come up with some other way).**\n",
    "\n",
    "- **For each of these hypothetical parameter choices, compute $I(\\mathbf{y},\\hat{\\mathbf{y}},\\mathbf{w})$.**\n",
    "\n",
    "- **Identify which hypothetical $\\mathbf{w}$ gives the lowest $I$.**\n",
    "\n",
    "- **Make a plot with $w_0$ and $w_1$ on the first and second axis, with points colored by the associated value of $I$.  Make sure you highlight what you found to be the optimal value of parameters.**\n",
    "\n",
    "- **Use this information to predict the IMR for Venzuela.  Make another plot showing the data we used to make this prediction, as well as the prediction itself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IC1B.  Let's be less stupid.\n",
    "Answer the following question:\n",
    "- **Why doesn't the brute force method work if we have more parameters?**\n",
    "\n",
    "What is the alternative?  Recall from basic calculus the method for finding so-called *extremal points*: minima and maxima.  What was the condition for a function to be minimal?  It's derivative had to be zero.  We can use this method to solve this problem as well.\n",
    "\n",
    "We seek to find the minimum value of $I(\\mathbf{y},\\hat{\\mathbf{y}},\\mathbf{w})$ with respect to the values of $\\mathbf{w}$.  To do this, we can simply compute its *partial derivatives*\n",
    "$$\n",
    "\\frac{\\partial I}{\\partial w_0}, \\frac{\\partial I}{\\partial w_1}.\n",
    "$$\n",
    "If you're unfamiliar with partial derivatives, it's just like a normal derivative but where we treat every other variable as a constant.  For example:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} [x^2 + x y + y^2] = 2x + y.\n",
    "$$\n",
    "If we do this for our problem we have\n",
    "$$ \\frac{\\partial I}{\\partial w_0} = - \\sum_{i=1}^m (y_i - w_1 x_i - w_0)$$\n",
    "$$ \\frac{\\partial I}{\\partial w_1} = - \\sum_{i=1}^m (y_i - w_1 x_i - w_0)x_i. $$\n",
    "\n",
    "These values when taken together as a vector $[\\frac{\\partial I}{\\partial w_0},\\frac{\\partial I}{\\partial w_1}]$ are called the *gradient* of $I$, which we can annotate as $\\nabla I$.  We seek to make $I$ as small as possible, and the gradient tells us which way is uphill.  One way that we can use this information is to perform *gradient descent*.  The algorithm is easy:  while the magnitude of the gradient is greater than a certain tolerance, take a small step in the direction of the negative gradient\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - s \\nabla I.\n",
    "$$\n",
    "$s$ has to be selected manually: too small and we'll never reach the minimum.  Too large and we'll overshoot it.  You will very likely have to take many steps before reaching the minimum of $I$\n",
    "\n",
    "- **Implement the gradient descent method for finding the minimum of I**\n",
    "- **Compute the best values of $[w_0,w_1]$ using this method.  You'll likely need to try several values for $s$.  How does it compare to the estimate you produced using brute force.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IC1C. Let's be not stupid.\n",
    "The above two examples are useful for understanding the various ways in which cost functions can be minimized.  However, it's a little bit contrived for this problem: unlike most minimization problems, this one has an *exact* solution.  How do we compute it?\n",
    "\n",
    "Once again recall that at the minimum of $I$, $\\nabla I=0$.  As such, we set the expressions for each derivative of $I$ derived above to zero, and rearrange to get\n",
    "$$ \\sum_{i=1}^m (y_i 1) = \\sum_{i=1}^m w_1 x_i + \\sum_{i=1}^m w_0  $$.\n",
    "$$ \\sum_{i=1}^m (y_i x_i) = \\sum_{i=1}^m w_1 x_i^2 + \\sum_{i=1}^m w_0 x_i. $$\n",
    "If we assume that we can write a vector of ones of length $m$ as $\\mathbf{1}$, we can write this as the matrix equation\n",
    "$$\n",
    "\\left[ \\begin{array}{cc} \\mathbf{1} \\cdot \\mathbf{1} & \\mathbf{x} \\cdot \\mathbf{1} \\\\\n",
    "                      \\mathbf{1} \\cdot \\mathbf{x} & \\mathbf{x} \\cdot \\mathbf{x} \\end{array} \\right] \\left[ \\begin{array}{c} w_0 \\\\ w_1 \\end{array}\\right] = \\left[\\begin{array}{c} \\mathbf{1} \\cdot \\mathbf{\\hat{y}} \\\\\n",
    "                                                                  \\mathbf{x} \\cdot \\mathbf{\\hat{y}}. \\end{array}\\right]\n",
    "$$\n",
    "(Check for yourself, if you're interested).  If we define $\\mathbf{w} = [w_0,w_1]^T$ and \n",
    "$$X = \\left[ \\begin{array}{cc} 1 & x_0 \\\\ 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_m \\end{array} \\right],$$\n",
    "we can write this experession in matrix form\n",
    "$$ X^T X \\mathbf{w} = X^T \\mathbf{y}.$$\n",
    "$X^T X$ is a 2 by 2 matrix, and $X^T \\mathbf{y}$ is 2 by 1 vector, and we can easily solve this equation for $\\mathbf{w}$ using any technique for solving systems of linear equations.  This system of equations is called the *normal equations*\n",
    "\n",
    "- **Write a function that creates the matrix X (often called the Vandermonde Matrix).  Use this function to solve directly for $w_0$ and $w_1$.**  \n",
    "- **Compare this solution to the parameter estimates you produced using the brute force method.  Are they the same?  Why or why not?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IC1D. More complex models.\n",
    "With the result of IC1C in place, we can easily generalize to fitting models more complicated than a line.  For example, let's imagine that we thought the data were better fit by a *quadratic* model:\n",
    "$$y_i = w_0 + w_1 x_i + w_2 x_i^2$$\n",
    "The normal equations in fact still hold, but the definition of $X$ changes.  For the quadratic model,\n",
    "$$X = \\left[ \\begin{array}{cc} 1 & x_0 & x_0^2 \\\\ 1 & x_1 & x_1^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_m & x_m^2  \\end{array} \\right],$$\n",
    "and \n",
    "$$\n",
    "\\mathbf{w} = [w_0,w_1,w_2]^T.\n",
    "$$\n",
    "In fact, we can fit a polynomial of *arbitrarily high degree* using this method.  For a polynomial of degree $d$, we still have the normal equations\n",
    "$$\n",
    "X^T X \\mathbf{w} = X^T \\mathbf{y},\n",
    "$$\n",
    "but with\n",
    "$$X = \\left[ \\begin{array}{cc} 1 & x_0 & x_0^2 & \\cdots & x_0^d \\\\ 1 & x_1 & x_1^2 & \\cdots & x_1^d \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_m & x_m^2 & \\cdots & x_m^d  \\end{array} \\right],$$\n",
    "and \n",
    "$$\n",
    "\\mathbf{w} = [w_0,w_1\\cdots,w_d]^T.\n",
    "$$\n",
    "\n",
    "- **Implement a method that allows you to fit a polynomial of degree $d$ to our dataset.  To do this you will need to generalize your method for creating the Vandermonde matrix to accept an additional argument $d$ that controls the *model complexity*. **\n",
    "- **Fit (and plot) a polynomial of 10th degree.  How does the prediction for Venezuela look?  15th degree?**\n",
    "- **Is fitting these very complex models a good idea?  Why or why not?  If not, what should we do about it? **|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
